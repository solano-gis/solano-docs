---
title: "The Shared Canvas"
description: "How AI and user collaborate on a single interactive map — drawing, annotating, and reasoning together"
---

Most AI systems have a one-way relationship with maps: the AI generates an image, the user looks at it. SAGE inverts this. The AI and the user share a single live ArcGIS map — the same pixels, the same layers, the same state. Neither has a private view. This page explains why that matters and how it works.

## The core idea

The AI doesn't have its own GIS renderer. It commands the same map the user sees via an iframe postMessage bridge. After every command, the map captures a screenshot and sends it back — this is how the AI "sees" what it did.

```
┌─────────────────────────────────────────────────────────┐
│  sage-web (chat UI)                                     │
│                                                         │
│   User types message ──→ AI decides to draw ──→         │
│   postMessage('annotate', polygons) ──────────────→     │
│                                                    │    │
│   ┌───────────────────────────────────────────┐    │    │
│   │  sage-map (iframe)                        │◄───┘    │
│   │                                           │         │
│   │   Renders annotation on GraphicsLayer     │         │
│   │   Captures screenshot (1092×1092 JPEG)    │         │
│   │   Sends command-ack + screenshotDataUrl ──────→     │
│   └───────────────────────────────────────────┘         │
│                                                         │
│   AI receives screenshot ──→ describes what it sees     │
└─────────────────────────────────────────────────────────┘
```

This creates a feedback loop: the AI draws, sees the result, and adjusts. The user watches it happen in real time.

## Two drawing systems, one map

The shared canvas has two independent drawing systems layered on the same map:

| | AI annotations | User freehand |
|---|---|---|
| **Triggered by** | AI tool call (`annotate_map`) | User clicks pencil button |
| **Shapes** | Markers, lines, polygons, circles, labels | Freehand polylines |
| **Color** | 6-color palette (crimson, emerald, cobalt, violet, teal, slate) | Orange (SAGE brand) |
| **Style** | Hand-drawn jitter (subdivide + perturb) | Smooth freehand strokes |
| **Persistence** | Cleared by AI or on new annotation set | Persists until user clears |
| **Rendered on** | Annotation GraphicsLayer | Sketch GraphicsLayer |
| **AI awareness** | AI drew it — full awareness | AI receives stroke summaries + screenshot |

The color separation is intentional. Orange is reserved exclusively for user actions (freehand drawing, parcel selection, feature highlights). The AI's six-color palette never includes orange, so it's always clear who drew what.

## AI annotations

When the AI calls `annotate_map`, sage-web forwards the annotation array to the iframe via the `annotate` command. sage-map renders each annotation on a dedicated `GraphicsLayer`:

### Color palette

The AI chooses from six constrained colors instead of arbitrary hex values. This prevents clashing with the orange highlight system and keeps visuals consistent across sessions.

| Color | Hex | Typical use |
|-------|-----|-------------|
| `crimson` | `#DC2626` | Boundaries, danger zones, warnings |
| `emerald` | `#059669` | Natural areas, parks, vegetation |
| `cobalt` | `#2563EB` | Water, infrastructure, default |
| `violet` | `#7C3AED` | Special or unique features |
| `teal` | `#0D9488` | Alternative to green |
| `slate` | `#475569` | Subtle reference lines |

### Fill behavior

Polygons and circles default to **outline-only** — no fill. The AI sets `filled: true` when area shading helps communicate extent (e.g., shading a flood zone or park boundary). Filled regions render at 18% opacity so underlying map detail remains visible.

### Hand-drawn jitter

AI-generated geometry is mathematically perfect — straight edges, exact angles. This looks authoritative in a way that's misleading, since the AI is approximating boundaries from memory, not querying survey-grade data.

The renderer applies a two-step post-process to produce an Excalidraw-style sketchy appearance:

1. **Subdivide** — Each edge is split into segments roughly 20 pixels long. An AI polygon with 5 corners becomes a path with 50+ intermediate points.
2. **Jitter** — Each intermediate point is randomly offset by ±4 pixels (scaled to map resolution so it looks consistent at any zoom level).

The jitter uses a seeded PRNG derived from the path coordinates, so the same geometry always produces the same wobble — no flicker when the map re-renders.

First and last vertices of each path are kept exact so polygons close cleanly.

<Note>
The jitter only applies to AI annotations. User freehand strokes are already organic by nature and are rendered as-is. Data-layer highlights (parcels, zoning districts) also bypass jitter — those use exact geometry from the GIS server.
</Note>

### Annotation groups

Each `annotate` call can include a `groupId`. The AI uses this to manage independent sets of annotations:

```
annotate_map({ id: "flood-zones", annotations: [...] })    // Draw flood zones
annotate_map({ id: "fire-zones", annotations: [...] })     // Draw fire zones (flood zones still visible)
annotate_map({ id: "flood-zones", annotations: [...] })    // Replace flood zones only
```

Clearing by group ID is surgical — `clear-annotations` with `id: "flood-zones"` removes only that set.

## User freehand drawing

The pencil button (embed mode only) activates freehand polyline drawing. The user draws directly on the map — circling areas, pointing at features, sketching boundaries — then sends a message to ask about what they drew.

### Draw-to-ask workflow

```
1. User clicks pencil button → draw mode active (button turns orange)
2. User draws one or more freehand strokes on the map
3. Each completed stroke fires 'stroke-added' event (lightweight, no screenshot)
4. sage-web shows stroke count + clear button above the chat input
5. User types a question and sends
6. sage-web calls 'get-sketch-summary' → iframe computes per-stroke bbox + screenshot
7. Stroke summaries + screenshot injected into AI context as dynamic system prompt
8. AI reasons about the drawings using both geometry and vision
```

### Per-stroke summaries

Each freehand stroke is summarized with geographic metadata:

```
StrokeSummary {
  bbox: { north, south, east, west }    // Bounding box in degrees
  centroid: { latitude, longitude }      // Center of the bbox
}
```

This gives the AI spatial coordinates without sending raw polyline vertices. The AI uses the bounding boxes for coarse location (e.g., "this circle is around central Fairfield") and the screenshot for visual context (e.g., "I can see you've circled a cluster of parcels near the highway").

### Persistence model

Unlike AI annotations (which the AI can clear and redraw), user freehand drawings persist until the user explicitly clears them via the trash button. This is intentional:

- The user may draw multiple strokes across multiple messages
- Clearing would lose context the user is building up
- The AI should reason about the cumulative drawing, not just the latest stroke

## Three layers of visual communication

The shared canvas stack, from bottom to top:

| Layer | Contents | Who controls | Color system |
|-------|----------|-------------|--------------|
| ArcGIS data layers | Parcels, zoning, aerials, hazards | AI (layer toggling) + user (layer list) | Layer symbology (varied) |
| Highlight layer | Parcels by APN, features by WHERE | AI (`highlight-apns`, `highlight-features`) | Orange outlines/markers |
| Annotation layer | AI-drawn markers, lines, polygons, labels | AI (`annotate_map`) | 6-color palette with jitter |
| Sketch layer | User freehand strokes | User (pencil button) | Orange freehand lines |

Each layer has a distinct visual language. A user looking at the map can immediately tell:
- **Crisp orange outlines** = real GIS data the AI queried
- **Wobbly colored lines** = AI sketching from approximate knowledge
- **Smooth orange freehand** = something the user drew

## Why this matters

### No divergence

In a typical chat-with-a-map system, the AI has a separate rendering pipeline. It generates a static image, the user looks at it, and if they want changes, they describe them in text. The AI and user are looking at different things.

With the shared canvas, there is no divergence. When the AI says "I've outlined the marsh area in green," the user can see the exact outline. When the user draws a circle and asks "what's in here?", the AI sees the exact circle. Both parties work with the same visual truth.

### AI self-correction

Because the AI receives screenshots of its own drawings, it can self-correct. If it draws a polygon that misses part of an area (visible in the screenshot), it can acknowledge the gap and offer to redraw. This is only possible because the AI sees the same map the user does.

### Visual honesty

The hand-drawn jitter is a deliberate design choice. When the AI sketches "the Suisun Marsh area," it's working from approximate knowledge — not querying a marsh boundary layer. The wobbly lines communicate this imprecision honestly. Compare:

- **Crisp polygon** → implies exact, authoritative data (misleading for AI-generated geometry)
- **Sketchy polygon** → implies "roughly this area" (honest about the AI's uncertainty)

Meanwhile, actual GIS data (parcel boundaries, zoning districts, flood zones) renders with exact geometry and orange highlighting — visually distinct and clearly authoritative.

## Implementation details

For protocol-level details, see:
- [Map Protocol](/integration/map-protocol) — postMessage command/event registry, annotation types, sketch summary payload
- [sage-map service docs](/services/sage-map) — rendering implementation, color palette, file structure
- [sage-web service docs](/services/sage-web) — tool schema, context injection, screenshot pipeline
